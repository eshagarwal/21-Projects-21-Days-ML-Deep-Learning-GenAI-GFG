# ğŸŒ¸ Transfer Learning with Pre-trained Models on the Oxford Flowers 102 dataset

## ğŸ“‹ Overview
This project demonstrates transfer learning using popular pre-trained CNN architectures (ResNet50, VGG16, MobileNetV2) on the Oxford Flowers 102 dataset. The goal is to classify 102 different flower species by leveraging models pre-trained on ImageNet. ğŸš€

## ğŸ“Š Dataset
- **Oxford Flowers 102**: Contains 102 flower categories with images of varying sizes ğŸŒº
- **Classes**: 102 different flower species ğŸŒ»
- **Image Format**: RGB images resized to 224x224 pixels ğŸ“¸

## ğŸ¤– Models Implemented
1. **ResNet50**: Deep residual network with skip connections ğŸ”—
2. **VGG16**: Classic deep CNN with small convolution filters ğŸ§ 
3. **MobileNetV2**: Lightweight model using depthwise separable convolutions ğŸ“±

## âœ¨ Key Features
- **Transfer Learning**: Pre-trained ImageNet weights with frozen base layers ğŸ§Š
- **Data Augmentation**: Random flips, rotations, translations, and zoom ğŸ”„
- **Custom Classification Head**: Dense layers with batch normalization and dropout ğŸ¯
- **Model Comparison**: Evaluation using F1-score and accuracy metrics ğŸ“ˆ

## ğŸ“ Project Structure
```
DAY 9/
â”œâ”€â”€ transfer_learning_computer_vision.ipynb  # Main notebook ğŸ““
â””â”€â”€ README.md                               # This file ğŸ“
```

## ğŸ“Š Results
The project compares the performance of all three models on the test dataset, evaluating:
- **F1 Score (Macro)**: For handling class imbalances âš–ï¸
- **Accuracy**: Overall classification performance ğŸ¯
- **Loss**: Training convergence ğŸ“‰

## ğŸ› ï¸ Requirements
- TensorFlow/Keras ğŸ”§
- TensorFlow Datasets ğŸ“¦
- NumPy, Pandas ğŸ¼
- Matplotlib, Seaborn ğŸ“Š

## ğŸš€ Usage
1. Open the Jupyter notebook ğŸ“–
2. Run cells sequentially to:
   - Load and preprocess the Oxford Flowers 102 dataset ğŸ“¥
   - Build and train the three models ğŸ—ï¸
   - Evaluate and compare performance ğŸ“Š

## ğŸ’¡ Key Takeaways
- Transfer learning significantly reduces training time and computational requirements âš¡
- Pre-trained models provide excellent feature extraction capabilities ğŸ”
- Data augmentation helps improve generalization on smaller datasets ğŸ“ˆ
- Different architectures show varying performance characteristics for flower classification ğŸŒ¸

---
**Day 9**: Transfer Learning with Pre-trained Models ğŸ“